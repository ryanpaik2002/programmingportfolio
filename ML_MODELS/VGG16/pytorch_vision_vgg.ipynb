{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_vision_vgg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python38564bitpytorchpipenvd16d7860d73c4f28a1fb12fd9cc98f62",
      "display_name": "Python 3.8.5 64-bit ('pytorch': pipenv)",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9KWS_T6xda8"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# vgg-nets\n",
        "\n",
        "*Author: Pytorch Team*\n",
        "\n",
        "**Award winning ConvNets from 2014 Imagenet ILSVRC challenge**\n",
        "\n",
        "<img src=\"https://pytorch.org/assets/images/vgg.png\" alt=\"alt\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXWpxSanxdcW"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=True)\n",
        "# or any of these variants\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11_bn', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13_bn', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16_bn', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /home/swingology/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /home/swingology/.cache/torch/hub/checkpoints/vgg11-bbd30ac9.pth\n",
            "100%|██████████| 507M/507M [00:24<00:00, 21.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk-GUzqpxdca"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "Here's a sample execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qwl8jOmxdcd"
      },
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jtft4yCxdcg",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "print(output[0])\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1535e-01, -5.6933e-01, -9.2386e-01,  1.6341e-03,\n        -1.1033e+00,  1.2276e+00,  4.6590e+00,  1.4872e+00,  6.1385e-01,\n         3.5586e-01,  2.7449e-01,  1.3661e+00, -9.8060e-01,  1.5219e+00,\n         1.0632e+00,  6.0680e-02,  1.4753e+00, -1.7704e+00,  8.7646e-01,\n        -1.2087e+00, -6.8658e-01,  3.5955e-01,  4.6501e-01, -1.3062e+00,\n        -3.2562e-01, -2.4099e+00,  3.2876e-02,  6.2888e-01,  2.6916e+00,\n         1.3904e+00, -1.5313e+00, -1.9953e+00,  2.2727e+00,  2.8389e-01,\n         1.2602e+00, -3.4825e-01, -7.3350e-01, -5.8836e-02,  4.3050e-01,\n         1.7007e+00, -1.3030e+00,  9.2081e-01,  3.6050e+00, -6.1280e-02,\n        -1.0152e-01,  3.6731e-01, -1.8321e+00,  1.5109e-01, -1.4931e+00,\n        -5.2439e-01,  1.1890e+00, -2.0661e+00, -4.6549e-01, -9.6388e-01,\n         6.8993e-01,  1.0780e+00,  6.9978e-01,  5.7481e-02, -6.8061e-01,\n         5.7220e-01,  1.5986e+00,  2.3552e+00,  1.5035e+00,  3.0943e-01,\n        -1.4733e-01,  1.9637e+00, -2.3652e+00, -3.6162e-01,  3.1390e-01,\n         1.0663e+00, -4.3093e+00,  8.8931e-01, -2.1141e-01,  1.8497e-01,\n         2.8303e+00,  1.5080e+00,  1.4758e+00, -9.2225e-01, -2.4982e-01,\n        -3.0066e-02,  1.6911e+00,  1.5674e+00,  4.9450e-01,  9.6069e-01,\n         6.1360e-01,  2.0714e+00,  1.3681e+00, -1.6175e+00, -2.3618e+00,\n         7.5634e-01,  2.4257e-01,  1.4980e+00,  1.8966e-01, -6.8900e-01,\n         6.9403e-01,  4.8064e-01, -4.8298e-01,  5.3141e-01,  9.4851e-01,\n        -1.8786e+00,  2.1545e+00,  2.1849e-02,  4.9053e-01, -2.1115e+00,\n        -1.5733e+00,  1.0163e+00, -3.5886e-01,  6.0910e-01,  3.4384e-01,\n         5.4553e-01,  5.7310e-01,  2.9096e+00,  2.5206e-01,  1.0148e+00,\n         1.2127e+00, -1.1031e+00, -1.9798e-02,  3.2756e-01,  2.5397e+00,\n         8.5147e-01, -5.3103e-01,  1.7064e+00,  1.3856e+00, -1.2248e-01,\n         1.1745e+00, -1.4808e+00,  1.6042e+00,  2.4624e+00,  2.8299e-01,\n         2.0366e-01,  4.7452e+00,  1.2952e-02,  1.6321e+00,  2.5710e-01,\n         1.6973e+00,  2.1780e+00,  6.1313e-01, -7.9477e-01, -7.5901e-01,\n        -1.6023e+00, -2.3833e+00, -2.1137e+00, -1.1440e-01, -7.8111e-01,\n        -1.5605e+00, -1.2960e+00, -1.1605e+00,  2.6313e-02,  1.1576e+00,\n        -2.9653e-01,  2.0287e-01,  1.0046e+00,  1.2345e+00, -4.6265e-01,\n         4.3824e+00,  2.2766e-01,  4.3463e-01,  8.1034e-01, -4.3273e-01,\n         1.5598e+00,  3.6004e+00, -9.5920e-01,  3.6852e-02, -1.3355e-01,\n        -2.8851e-01, -3.2901e-01, -1.0845e+00, -2.9822e-01, -1.4175e+00,\n        -1.3566e+00,  2.9240e+00, -2.7468e-01,  2.9706e-01,  2.3774e+00,\n         3.0144e-01,  1.3111e+00,  3.0202e+00,  2.9869e+00, -4.6783e-01,\n         3.9860e-01, -3.1481e+00, -1.1129e-01,  3.2922e+00, -1.4170e+00,\n        -9.4432e-01,  3.4433e+00, -1.2246e+00,  1.5672e+00,  1.2806e+00,\n         1.7724e+00, -3.5041e-01,  3.2952e-01,  1.5174e-01, -3.3466e+00,\n        -7.6844e-02, -3.5567e-01,  3.3316e+00, -5.9724e-01,  2.8310e+00,\n        -1.0374e+00,  4.4040e-01,  1.4977e+00,  1.5398e+00,  6.5892e-01,\n         3.3637e+00, -8.7391e-01, -7.9642e-01,  1.8813e+00,  2.0770e+00,\n         3.0193e-01,  1.0954e+00,  2.7271e+00, -9.5439e-01, -2.0770e+00,\n         9.0616e-01,  3.6145e+00, -2.9552e-02, -2.0299e+00, -1.0145e+00,\n         2.6187e+00, -1.5192e+00,  7.9266e-01,  1.2262e+00,  1.6123e+00,\n         8.9087e-01, -4.4171e-02, -2.6655e-01,  2.2010e-01, -8.0623e-01,\n         8.9035e-02, -9.2230e-01, -1.3389e+00,  2.1673e+00,  1.0118e+00,\n         7.3924e-02, -2.0271e-01,  2.2924e+00,  9.5221e-01,  8.5869e-01,\n         1.3071e+00, -3.0238e-01, -1.0606e+00,  1.9791e-01,  6.0690e-01,\n         1.6076e+00,  1.0922e+00,  2.2457e+00, -4.3654e-01,  5.1129e-01,\n         1.2216e+00, -1.2697e+00,  2.2555e+00, -6.1005e-01,  2.9440e+00,\n        -8.6731e-01, -1.8347e+00,  1.1608e+00, -9.4434e-01,  1.3970e+00,\n         5.5189e+00,  2.0553e+00,  1.2961e+00,  6.1223e-01,  7.1140e-01,\n         1.1620e-01,  3.5554e+00, -3.8158e-02,  1.0893e+00,  1.1514e+00,\n        -2.5383e+00, -1.1439e+00, -6.0837e-01,  9.9515e-01, -9.2089e-02,\n         7.1134e-01, -2.1322e+00,  2.2873e-01,  1.9760e+00,  1.5373e+00,\n        -8.5339e-02,  4.0005e-01,  1.4779e+00,  1.8500e+00, -5.2082e-01,\n         1.3380e+00,  2.0815e+00,  1.4752e+00, -1.3105e+00, -5.4467e-03,\n        -3.0152e-01,  6.3481e-01,  3.1567e+00,  2.0476e+00, -2.3095e+00,\n         3.1713e+00,  8.6629e-01,  5.7449e-01,  3.2303e-01,  6.9610e-01,\n         1.3289e+00,  1.1970e+00, -2.2083e+00,  9.4129e-02, -1.1948e+00,\n         7.4428e-01,  1.1327e+00,  8.2615e+00, -7.7053e-01, -8.0219e-01,\n        -1.9831e-01, -2.4955e+00, -9.4234e-01,  4.2280e-01, -5.3669e-01,\n        -4.6969e-01,  3.0146e+00,  1.3154e+00, -1.2183e+00, -7.8476e-01,\n        -1.7225e-01, -1.4957e-01,  1.4632e+00,  5.3539e-02,  4.3476e-01,\n         2.5400e+00, -7.2034e-01,  2.3579e-01, -1.3633e+00, -1.1853e-02,\n        -9.2088e-01,  4.1686e+00,  7.8467e-01, -6.6840e-01,  3.1012e+00,\n         8.7895e-01, -3.7155e-01,  2.1289e+00, -1.3988e+00, -1.3876e+00,\n         2.6844e-01, -1.6460e-02, -9.3011e-01, -8.2712e-01, -2.3454e+00,\n        -2.8525e-01,  1.7187e-01,  2.8446e-01, -1.4247e+00,  6.4067e-01,\n        -3.6989e-01,  1.5466e+00,  7.2572e-01,  2.1174e+00,  1.5596e+00,\n        -7.7273e-01, -2.6335e+00,  1.1529e+00, -2.7468e-01,  2.8754e+00,\n         5.0994e-01, -1.3668e+00,  8.3357e-01,  1.4380e+00,  1.7188e+00,\n        -8.0787e-01,  1.8175e-01,  2.0904e+00,  2.4696e-01,  1.9308e-01,\n         1.9083e-01,  1.4609e-01, -6.5661e-01,  1.8480e+00,  1.4133e-01,\n         1.1344e-01, -1.0328e+00, -2.8336e+00, -3.4011e-01, -5.1320e-01,\n        -1.2627e+00, -1.6173e+00, -2.2523e-01, -4.3688e-01,  4.5332e+00,\n        -4.0637e+00, -2.6290e+00, -1.1482e+00, -2.2463e+00, -1.1781e+00,\n         1.7541e+00,  1.4279e-01,  2.9286e+00, -9.0943e-01, -2.3186e-01,\n        -3.5575e-01, -2.5918e+00,  4.1308e-02,  5.5204e-01, -2.6511e+00,\n        -1.5283e+00, -1.0126e+00, -3.0479e+00,  1.5293e+00,  1.9839e-01,\n        -1.6118e-01, -1.2514e-01, -2.2566e+00,  9.9114e-01, -1.8581e+00,\n        -2.2367e-01, -8.1847e-01, -3.9367e-01,  1.8342e+00, -7.6765e-01,\n        -3.1840e+00,  1.7643e-01, -1.4945e+00, -2.1394e+00, -2.6717e+00,\n        -2.4395e+00, -9.9942e-01, -2.3428e+00,  1.9559e+00, -3.0102e-01,\n         3.1331e-02,  1.1696e+00, -1.5993e+00, -1.3693e+00, -2.5113e-01,\n         2.8267e+00, -9.7870e-01,  2.5997e+00,  1.7431e+00, -9.2974e-01,\n        -7.5014e-03, -2.5817e-01,  2.5298e+00, -1.6849e+00,  1.8899e+00,\n        -1.7109e-01, -2.5225e+00,  2.3292e+00, -1.5661e+00, -1.5559e+00,\n        -2.3387e-01, -1.6866e+00, -1.1285e+00, -3.7275e+00, -4.6460e+00,\n        -6.7705e-01, -2.2285e+00, -4.1949e+00,  1.7822e+00,  5.0164e+00],\n       device='cuda:0')\ntensor([6.5719e-07, 5.4257e-07, 8.3762e-07, 2.3143e-07, 6.2669e-07, 9.2401e-07,\n        6.2293e-07, 4.2898e-05, 1.7197e-04, 1.8667e-07, 2.5973e-08, 1.8266e-07,\n        3.1730e-08, 1.1225e-07, 1.9245e-07, 2.6807e-07, 1.0789e-06, 2.2364e-06,\n        1.1568e-06, 5.5911e-08, 9.6364e-08, 7.8387e-07, 1.0305e-06, 7.4661e-07,\n        1.3821e-07, 2.9495e-07, 1.2256e-07, 4.9977e-07, 3.6389e-07, 2.3018e-06,\n        1.0086e-06, 1.0737e-07, 1.2553e-07, 4.4497e-08, 1.1147e-07, 1.4412e-07,\n        1.1462e-07, 1.1333e-07, 4.3609e-08, 6.0233e-07, 2.9004e-07, 5.5672e-08,\n        4.9030e-08, 6.3222e-08, 1.9436e-07, 1.2477e-07, 1.1255e-06, 1.0951e-07,\n        7.9355e-08, 3.1663e-07, 7.6082e-07, 9.8748e-07, 3.3621e-07, 6.6000e-08,\n        1.6087e-07, 2.0891e-07, 2.0560e-07, 4.5049e-08, 8.8260e-08, 2.3444e-07,\n        3.4993e-07, 4.5770e-08, 4.0833e-08, 3.6222e-08, 1.3470e-07, 7.2037e-08,\n        2.6326e-07, 2.7931e-07, 6.6326e-08, 2.6345e-08, 6.1099e-08, 3.3986e-07,\n        1.1419e-07, 1.6921e-07, 2.6144e-07, 1.3294e-07, 1.0496e-07, 2.4011e-07,\n        1.7512e-06, 3.7583e-07, 3.2038e-07, 6.5966e-07, 5.3803e-07, 1.0586e-06,\n        5.5582e-06, 4.6430e-07, 3.2259e-07, 2.2334e-07, 1.3871e-07, 8.8535e-06,\n        7.1712e-07, 4.6556e-08, 1.1458e-07, 4.5259e-08, 2.8080e-07, 3.5317e-08,\n        2.0983e-07, 4.2006e-07, 1.8965e-07, 7.0430e-06, 1.3522e-06, 1.0954e-08,\n        4.9346e-07, 1.1544e-08, 2.2575e-03, 1.2952e-06, 8.0622e-06, 1.4322e-07,\n        7.5071e-07, 3.5389e-07, 3.0294e-07, 1.9350e-08, 8.2695e-06, 6.7398e-07,\n        1.0956e-06, 2.3387e-06, 6.0282e-08, 1.5743e-07, 8.4535e-07, 1.2098e-07,\n        6.1666e-08, 2.7249e-07, 7.6087e-07, 1.0637e-07, 2.7546e-06, 4.2605e-07,\n        2.1230e-07, 1.3567e-06, 1.4938e-07, 4.3125e-07, 1.2849e-07, 1.6230e-07,\n        1.3014e-06, 6.7222e-08, 6.7548e-07, 2.8742e-07, 2.0122e-07, 2.8778e-08,\n        1.8104e-07, 1.7946e-07, 7.9578e-08, 1.1384e-07, 2.1683e-08, 4.7366e-07,\n        2.7406e-06, 7.6461e-07, 8.4856e-07, 9.3488e-08, 8.0591e-07, 8.3558e-08,\n        7.7531e-08, 2.9936e-03, 2.1027e-03, 7.2286e-05, 1.5371e-03, 1.0035e-05,\n        5.4352e-06, 4.8326e-03, 7.1874e-05, 8.6603e-07, 2.5820e-06, 4.9678e-07,\n        5.6999e-07, 6.9457e-07, 1.2725e-06, 5.5924e-07, 4.2144e-07, 2.4637e-07,\n        1.4098e-05, 2.6744e-05, 1.0846e-05, 3.6749e-07, 8.9838e-07, 4.7918e-05,\n        8.0440e-04, 6.2176e-06, 4.4018e-07, 1.0970e-06, 3.2682e-07, 1.3979e-05,\n        7.9859e-06, 2.9011e-07, 2.8485e-05, 4.4046e-06, 1.1004e-05, 3.5169e-04,\n        7.6505e-04, 1.3264e-05, 5.2542e-05, 5.1179e-06, 1.4345e-05, 1.6588e-06,\n        5.0986e-04, 2.3858e-04, 9.6440e-06, 3.0254e-05, 1.0155e-05, 9.3063e-06,\n        2.9906e-06, 1.0183e-03, 1.7175e-04, 2.1089e-05, 5.8533e-06, 4.3009e-03,\n        6.9836e-05, 4.8749e-06, 6.9907e-07, 3.0622e-04, 3.4423e-05, 5.6940e-06,\n        4.4053e-07, 1.6789e-06, 2.8235e-05, 3.9568e-06, 8.1039e-06, 1.3888e-05,\n        1.6753e-05, 1.0015e-05, 1.3530e-05, 5.3236e-06, 2.6664e-05, 1.6735e-06,\n        8.0616e-04, 3.0200e-03, 3.9904e-04, 1.0255e-05, 1.0290e-04, 3.5947e-04,\n        6.0863e-05, 1.5011e-04, 5.9829e-03, 1.7686e-02, 8.4329e-03, 1.3828e-05,\n        1.0605e-05, 1.1393e-03, 3.8575e-05, 4.8288e-06, 1.0827e-05, 8.6728e-05,\n        2.8296e-05, 3.9092e-06, 4.7569e-06, 8.9391e-07, 3.5709e-05, 3.6091e-05,\n        3.5243e-06, 1.0272e-04, 1.7759e-02, 2.9348e-03, 5.3407e-03, 2.8406e-05,\n        1.8973e-05, 2.0150e-05, 1.2317e-05, 1.5865e-05, 2.0625e-04, 9.5956e-03,\n        6.6737e-01, 1.6195e-01, 7.3903e-03, 1.7066e-02, 2.1040e-05, 1.8391e-03,\n        1.2181e-03, 4.4434e-05, 1.3025e-05, 2.6039e-05, 1.7291e-06, 2.6728e-04,\n        1.4873e-02, 1.9941e-05, 1.7843e-05, 9.3709e-05, 1.9599e-05, 2.5287e-06,\n        5.9497e-06, 5.4339e-05, 2.0393e-05, 5.4264e-03, 3.8285e-05, 1.3688e-04,\n        2.1217e-05, 8.9256e-04, 1.4812e-04, 1.1554e-04, 3.3087e-07, 2.8311e-04,\n        1.0096e-06, 1.3048e-06, 5.3786e-07, 2.0354e-06, 2.0888e-06, 2.0006e-06,\n        1.1352e-06, 2.2490e-06, 5.9250e-06, 3.7112e-07, 1.1306e-07, 7.8770e-07,\n        1.7605e-07, 4.2418e-07, 1.6997e-07, 1.0906e-07, 2.6900e-07, 1.1764e-07,\n        2.3781e-06, 2.0733e-07, 2.4110e-07, 1.0141e-06, 2.8619e-06, 4.2788e-07,\n        1.2111e-06, 4.0519e-07, 1.0956e-06, 3.3580e-07, 4.6999e-08, 5.0888e-07,\n        2.0739e-07, 1.2387e-07, 6.0243e-08, 4.7537e-07, 1.0905e-07, 1.2991e-07,\n        1.2188e-06, 7.0581e-07, 2.1215e-07, 5.1842e-06, 5.9857e-07, 1.9828e-06,\n        7.1348e-05, 1.4884e-04, 3.4301e-04, 5.7505e-05, 4.2507e-06, 5.7431e-06,\n        8.6785e-07, 1.0848e-06, 1.6810e-05, 1.5828e-06, 3.2778e-07, 4.6809e-06,\n        2.6759e-06, 2.1224e-07, 1.5544e-07, 2.4809e-06, 2.1444e-07, 8.8992e-07,\n        3.2652e-05, 9.4552e-08, 8.9179e-08, 3.7461e-08, 9.0636e-08, 2.4653e-07,\n        3.5862e-08, 7.5700e-05, 2.0259e-05, 3.1670e-06, 1.9127e-05, 2.8302e-05,\n        9.7562e-08, 5.7083e-05, 5.6871e-07, 3.7109e-07, 1.5720e-07, 2.9607e-08,\n        9.5437e-08, 9.5682e-08, 1.8850e-06, 1.6775e-07, 4.3938e-07, 4.7890e-07,\n        3.0999e-07, 4.9249e-06, 5.3504e-07, 7.1026e-08, 3.0223e-08, 3.9402e-06,\n        1.1748e-06, 3.1798e-07, 1.1780e-06, 2.0771e-07, 4.8292e-07, 1.1689e-06,\n        1.5391e-06, 1.3644e-08, 1.6930e-08, 9.3729e-07, 2.0483e-06, 1.4947e-06,\n        8.6825e-08, 1.9395e-06, 1.4675e-07, 8.9651e-08, 4.8329e-07, 5.9666e-07,\n        6.8305e-08, 6.4374e-07, 4.2880e-06, 2.7428e-07, 1.0254e-06, 2.4321e-06,\n        8.1098e-07, 2.4276e-07, 4.7157e-06, 1.3303e-06, 7.1072e-06, 1.1813e-05,\n        1.8815e-06, 1.9991e-05, 1.9213e-07, 3.8683e-06, 3.2639e-05, 3.5075e-06,\n        4.3592e-06, 5.0547e-06, 2.9374e-06, 9.1408e-06, 2.8725e-06, 2.8656e-06,\n        1.7019e-06, 3.8569e-06, 3.0595e-06, 7.7743e-07, 3.0921e-07, 1.0343e-05,\n        2.0879e-06, 1.8331e-06, 2.6454e-04, 8.1513e-06, 1.4930e-06, 2.5254e-05,\n        1.5075e-06, 2.2140e-06, 6.2779e-06, 3.5891e-05, 5.5783e-06, 5.3454e-06,\n        1.6998e-06, 8.1210e-06, 1.5038e-05, 5.9345e-07, 8.5608e-06, 2.1593e-05,\n        3.3110e-06, 2.1656e-06, 4.6619e-06, 1.2016e-06, 5.3858e-06, 5.4314e-06,\n        7.4448e-06, 2.9816e-06, 5.0779e-06, 1.0739e-05, 3.4399e-06, 2.1315e-05,\n        2.1092e-06, 3.5869e-05, 1.0555e-06, 2.2725e-06, 2.9654e-05, 3.1567e-07,\n        2.3504e-05, 1.0070e-03, 1.4974e-06, 5.3475e-06, 6.1792e-06, 1.0002e-07,\n        1.6494e-05, 6.4453e-06, 1.0988e-04, 1.6374e-06, 1.2957e-05, 2.8536e-07,\n        2.4321e-06, 9.6257e-07, 4.8710e-06, 1.2799e-06, 9.9153e-05, 5.7102e-06,\n        7.9779e-07, 8.2213e-07, 1.3202e-06, 2.7206e-06, 1.1070e-05, 2.1621e-06,\n        9.7673e-08, 2.9958e-06, 1.4164e-06, 5.1367e-05, 9.0614e-07, 2.3257e-06,\n        1.1548e-05, 2.5552e-06, 4.8911e-07, 2.2352e-06, 2.5859e-05, 2.8211e-05,\n        1.5971e-06, 8.3491e-07, 5.5230e-08, 2.9848e-06, 3.6339e-06, 5.4318e-06,\n        3.1990e-06, 1.0517e-06, 1.4181e-06, 3.2210e-06, 7.9376e-05, 1.0183e-06,\n        1.7721e-06, 2.2356e-06, 1.7181e-06, 2.7140e-06, 2.0788e-06, 4.4657e-05,\n        1.5643e-05, 9.2177e-07, 2.3693e-06, 3.4595e-05, 2.1058e-05, 1.4089e-05,\n        6.6861e-05, 4.1256e-06, 4.2727e-07, 4.2600e-07, 3.0715e-05, 1.6950e-05,\n        6.4768e-07, 6.3781e-06, 1.9773e-05, 4.7076e-06, 1.3885e-05, 2.7454e-06,\n        2.6986e-05, 1.5251e-07, 7.7286e-06, 1.1959e-04, 4.0651e-06, 1.2030e-03,\n        4.0073e-07, 1.3035e-06, 3.5751e-06, 4.8912e-05, 3.4189e-06, 3.2393e-06,\n        3.9940e-06, 1.6489e-06, 1.1567e-06, 2.9185e-06, 9.6675e-07, 9.9445e-06,\n        3.0750e-04, 1.2893e-05, 5.3832e-06, 4.1591e-06, 3.8341e-06, 1.1422e-05,\n        1.0929e-06, 1.3347e-05, 8.4375e-06, 3.0960e-06, 1.2740e-05, 4.9611e-07,\n        6.9999e-06, 8.6996e-07, 1.4665e-06, 4.1745e-06, 4.6388e-06, 7.8920e-07,\n        2.1040e-06, 2.6172e-07, 3.0111e-06, 5.4647e-06, 4.2991e-05, 1.1703e-05,\n        6.3012e-07, 3.9620e-07, 2.8278e-05, 3.8703e-06, 1.0274e-05, 2.0569e-06,\n        1.3992e-06, 2.7473e-06, 4.4814e-06, 1.5960e-05, 7.9167e-07, 7.3174e-06,\n        1.0717e-04, 2.7405e-06, 2.6324e-06, 4.2070e-06, 4.6643e-07, 3.3890e-06,\n        6.5465e-07, 1.7247e-06, 9.5682e-06, 3.6909e-07, 1.8293e-06, 1.1113e-06,\n        5.8088e-06, 8.5626e-06, 5.8663e-06, 3.0861e-06, 1.4752e-06, 5.1636e-06,\n        1.4412e-05, 3.0711e-05, 1.3104e-05, 3.9704e-06, 2.5146e-06, 2.0763e-05,\n        2.7368e-07, 2.0296e-06, 3.9882e-06, 8.4629e-06, 3.9171e-08, 7.0904e-06,\n        2.3585e-06, 3.5058e-06, 4.9390e-05, 1.3164e-05, 1.2746e-05, 1.1586e-06,\n        2.2696e-06, 2.8274e-06, 1.5808e-05, 1.3969e-05, 4.7776e-06, 7.6150e-06,\n        5.3819e-06, 2.3123e-05, 1.1445e-05, 5.7808e-07, 2.7463e-07, 6.2076e-06,\n        3.7136e-06, 1.3033e-05, 3.5222e-06, 1.4629e-06, 5.8326e-06, 4.7118e-06,\n        1.7976e-06, 4.9572e-06, 7.5228e-06, 4.4524e-07, 2.5128e-05, 2.9781e-06,\n        4.7587e-06, 3.5274e-07, 6.0417e-07, 8.0505e-06, 2.0352e-06, 5.3577e-06,\n        4.1094e-06, 5.0277e-06, 5.1683e-06, 5.3466e-05, 3.7490e-06, 8.0388e-06,\n        9.7976e-06, 9.6692e-07, 2.8566e-06, 4.0430e-06, 3.6933e-05, 6.8272e-06,\n        1.7133e-06, 1.6053e-05, 1.1647e-05, 2.5778e-06, 9.4305e-06, 6.6274e-07,\n        1.4493e-05, 3.4187e-05, 3.8668e-06, 3.5719e-06, 3.3516e-04, 2.9517e-06,\n        1.4903e-05, 3.7680e-06, 1.5907e-05, 2.5723e-05, 5.3793e-06, 1.3161e-06,\n        1.3640e-06, 5.8691e-07, 2.6878e-07, 3.5195e-07, 2.5988e-06, 1.3342e-06,\n        6.1196e-07, 7.9725e-07, 9.1293e-07, 2.9914e-06, 9.2723e-06, 2.1660e-06,\n        3.5691e-06, 7.9570e-06, 1.0014e-05, 1.8345e-06, 2.3319e-04, 3.6587e-06,\n        4.5000e-06, 6.5521e-06, 1.8903e-06, 1.3864e-05, 1.0668e-04, 1.1165e-06,\n        3.0231e-06, 2.5495e-06, 2.1835e-06, 2.0968e-06, 9.8506e-07, 2.1624e-06,\n        7.0605e-07, 7.5040e-07, 5.4242e-05, 2.2139e-06, 3.9216e-06, 3.1401e-05,\n        3.9388e-06, 1.0811e-05, 5.9717e-05, 5.7761e-05, 1.8251e-06, 4.3407e-06,\n        1.2510e-07, 2.6069e-06, 7.8382e-05, 7.0642e-07, 1.1333e-06, 9.1173e-05,\n        8.5625e-07, 1.3967e-05, 1.0486e-05, 1.7147e-05, 2.0524e-06, 4.0510e-06,\n        3.3912e-06, 1.0257e-07, 2.6982e-06, 2.0417e-06, 8.1534e-05, 1.6035e-06,\n        4.9422e-05, 1.0326e-06, 4.5260e-06, 1.3029e-05, 1.3589e-05, 5.6314e-06,\n        8.4195e-05, 1.2159e-06, 1.3139e-06, 1.9119e-05, 2.3253e-05, 3.9407e-06,\n        8.7134e-06, 4.4548e-05, 1.1219e-06, 3.6510e-07, 7.2109e-06, 1.0819e-04,\n        2.8289e-06, 3.8270e-07, 1.0564e-06, 3.9972e-05, 6.3778e-07, 6.4372e-06,\n        9.9311e-06, 1.4610e-05, 7.1015e-06, 2.7878e-06, 2.2320e-06, 3.6311e-06,\n        1.3011e-06, 3.1851e-06, 1.1585e-06, 7.6381e-07, 2.5450e-05, 8.0144e-06,\n        3.1373e-06, 2.3791e-06, 2.8841e-05, 7.5508e-06, 6.8766e-06, 1.0767e-05,\n        2.1534e-06, 1.0088e-06, 3.5514e-06, 5.3459e-06, 1.4542e-05, 8.6857e-06,\n        2.7527e-05, 1.8831e-06, 4.8585e-06, 9.8850e-06, 8.1852e-07, 2.7798e-05,\n        1.5831e-06, 5.5338e-05, 1.2240e-06, 4.6520e-07, 9.3022e-06, 1.1333e-06,\n        1.1780e-05, 7.2658e-04, 2.2753e-05, 1.0649e-05, 5.3745e-06, 5.9348e-06,\n        3.2728e-06, 1.0198e-04, 2.8047e-06, 8.6598e-06, 9.2153e-06, 2.3019e-07,\n        9.2824e-07, 1.5858e-06, 7.8821e-06, 2.6574e-06, 5.9345e-06, 3.4551e-07,\n        3.6626e-06, 2.1019e-05, 1.3554e-05, 2.6754e-06, 4.3470e-06, 1.2773e-05,\n        1.8531e-05, 1.7309e-06, 1.1105e-05, 2.3358e-05, 1.2739e-05, 7.8576e-07,\n        2.8979e-06, 2.1553e-06, 5.4973e-06, 6.8455e-05, 2.2579e-05, 2.8936e-07,\n        6.9458e-05, 6.9291e-06, 5.1755e-06, 4.0248e-06, 5.8447e-06, 1.1004e-05,\n        9.6453e-06, 3.2017e-07, 3.2013e-06, 8.8214e-07, 6.1332e-06, 9.0447e-06,\n        1.1281e-02, 1.3484e-06, 1.3064e-06, 2.3896e-06, 2.4026e-07, 1.1355e-06,\n        4.4471e-06, 1.7036e-06, 1.8217e-06, 5.9383e-05, 1.0857e-05, 8.6169e-07,\n        1.3293e-06, 2.4527e-06, 2.5090e-06, 1.2587e-05, 3.0740e-06, 4.5005e-06,\n        3.6945e-05, 1.4178e-06, 3.6885e-06, 7.4538e-07, 2.8794e-06, 1.1602e-06,\n        1.8830e-04, 6.3860e-06, 1.4934e-06, 6.4760e-05, 7.0174e-06, 2.0095e-06,\n        2.4492e-05, 7.1935e-07, 7.2750e-07, 3.8109e-06, 2.8662e-06, 1.1495e-06,\n        1.2742e-06, 2.7916e-07, 2.1906e-06, 3.4602e-06, 3.8725e-06, 7.0100e-07,\n        5.5296e-06, 2.0128e-06, 1.3681e-05, 6.0204e-06, 2.4212e-05, 1.3860e-05,\n        1.3454e-06, 2.0929e-07, 9.2291e-06, 2.2139e-06, 5.1670e-05, 4.8519e-06,\n        7.4275e-07, 6.7060e-06, 1.2274e-05, 1.6253e-05, 1.2990e-06, 3.4945e-06,\n        2.3567e-05, 3.7300e-06, 3.5343e-06, 3.5264e-06, 3.3721e-06, 1.5111e-06,\n        1.8494e-05, 3.3561e-06, 3.2637e-06, 1.0374e-06, 1.7133e-07, 2.0737e-06,\n        1.7441e-06, 8.2423e-07, 5.7817e-07, 2.3261e-06, 1.8824e-06, 2.7113e-04,\n        5.0073e-08, 2.1024e-07, 9.2429e-07, 3.0824e-07, 8.9705e-07, 1.6837e-05,\n        3.3610e-06, 5.4490e-05, 1.1735e-06, 2.3108e-06, 2.0415e-06, 2.1820e-07,\n        3.0366e-06, 5.0606e-06, 2.0564e-07, 6.3199e-07, 1.0585e-06, 1.3828e-07,\n        1.3446e-05, 3.5531e-06, 2.4800e-06, 2.5710e-06, 3.0507e-07, 7.8505e-06,\n        4.5445e-07, 2.3298e-06, 1.2853e-06, 1.9655e-06, 1.8240e-05, 1.3523e-06,\n        1.2069e-07, 3.4760e-06, 6.5372e-07, 3.4302e-07, 2.0144e-07, 2.5409e-07,\n        1.0725e-06, 2.7988e-07, 2.0601e-05, 2.1563e-06, 3.0065e-06, 9.3845e-06,\n        5.8868e-07, 7.4093e-07, 2.2667e-06, 4.9213e-05, 1.0950e-06, 3.9216e-05,\n        1.6652e-05, 1.1499e-06, 2.8920e-06, 2.2508e-06, 3.6572e-05, 5.4039e-07,\n        1.9285e-05, 2.4555e-06, 2.3385e-07, 2.9922e-05, 6.0858e-07, 6.1482e-07,\n        2.3061e-06, 5.3948e-07, 9.4263e-07, 7.0083e-08, 2.7971e-08, 1.4805e-06,\n        3.1379e-07, 4.3915e-08, 1.7315e-05, 4.3957e-04], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55nrCDjpxdci"
      },
      "source": [
        "# Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-02-03 00:08:26--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.24.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.24.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt’\n",
            "\n",
            "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-02-03 00:08:27 (1.01 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FbvLUK5xdcl"
      },
      "source": [
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "# Show top categories per image\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "for i in range(top5_prob.size(0)):\n",
        "    print(categories[top5_catid[i]], top5_prob[i].item())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samoyed 0.667373538017273\nPomeranian 0.1619524508714676\nEskimo dog 0.017759360373020172\ncollie 0.017686208710074425\nkeeshond 0.01706552878022194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGzR8k4Ixdco"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "Here we have implementations for the models proposed in [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556),\n",
        "for each configurations and their with bachnorm version.\n",
        "\n",
        "For example, configuration `A` presented in the paper is `vgg11`, configuration `B` is `vgg13`, configuration `D` is `vgg16`\n",
        "and configuration `E` is `vgg19`. Their batchnorm version are suffixed with `_bn`.\n",
        "\n",
        "Their 1-crop error rates on imagenet dataset with pretrained models are listed below.\n",
        "\n",
        "| Model structure | Top-1 error | Top-5 error |\n",
        "| --------------- | ----------- | ----------- |\n",
        "|  vgg11          | 30.98       | 11.37       |\n",
        "|  vgg11_bn       | 26.70       | 8.58        |\n",
        "|  vgg13          | 30.07       | 10.75       |\n",
        "|  vgg13_bn       | 28.45       | 9.63        |\n",
        "|  vgg16          | 28.41       | 9.62        |\n",
        "|  vgg16_bn       | 26.63       | 8.50        |\n",
        "|  vgg19          | 27.62       | 9.12        |\n",
        "|  vgg19_bn       | 25.76       | 8.15        |\n",
        "\n",
        "### References\n",
        "\n",
        "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}